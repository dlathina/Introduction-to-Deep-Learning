{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1FGx1BW52BC"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHso03c1gZUS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWAJjSXtgmHr"
   },
   "outputs": [],
   "source": [
    "# buat fungsi sigmoid\n",
    "def sigmoid(x):\n",
    "  return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wddxO_f2g2mR"
   },
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "# hyperparameter adalah parameter dimana kita dapat menentukan nilai epoch dan alpha (learning rate)\n",
    "# train parameter adalah parameter hasil dari pelatihan dari machine learning\n",
    "\n",
    "EPOCHS = 100\n",
    "ALPHA = 0.01\n",
    "\n",
    "# alpha adalah langkah untuk mencari minimum fungsi loss (gradient descent)\n",
    "\n",
    "# generate data\n",
    "(X,y)=make_blobs(n_samples=250,n_features=2, centers=2, cluster_std=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "o90idhsCiIeE",
    "outputId": "5c21f4fc-cea9-4064-9d4f-57d83dcb4247"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.61941797,  6.75120539],\n",
       "       [-5.42507161,  8.56631612],\n",
       "       [-6.96700861,  5.44531279],\n",
       "       [ 6.99974881,  2.90089274],\n",
       "       [-7.58939614,  6.64313903]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1F5gBNoriPTS"
   },
   "outputs": [],
   "source": [
    "# tambahkan bias term pada X\n",
    "X=np.c_[np.ones((X.shape[0])), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "ZLiHXtLPjB22",
    "outputId": "549fa4ef-f291-418d-a359-72cd0f2eb91c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -3.61941797,  6.75120539],\n",
       "       [ 1.        , -5.42507161,  8.56631612],\n",
       "       [ 1.        , -6.96700861,  5.44531279],\n",
       "       [ 1.        ,  6.99974881,  2.90089274],\n",
       "       [ 1.        , -7.58939614,  6.64313903]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcMnYodKjLjZ"
   },
   "outputs": [],
   "source": [
    "# inisialisasi Weight (W) dan list loss\n",
    "\n",
    "# andaikan W adalah nilai random dari kumpulan angka yang menyebar uniform (seragam)\n",
    "W=np.random.uniform(size=X.shape[1],)\n",
    "# note : tambahkan ',' dibelakang, agar W menjadi format tuple, bukan float. \n",
    "\n",
    "# define losshistory sebagai array kosong\n",
    "lossHistory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BumqEzRNjcQA",
    "outputId": "3e915211-babc-495b-8b2d-02cc75cffc5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18422753, 0.63483444, 0.06703088])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_INzbkc3j4L3"
   },
   "outputs": [],
   "source": [
    "# loop training sesuai epochs\n",
    "for epoch in range(EPOCHS):\n",
    "  pred = sigmoid (X.dot(W))\n",
    "# X.dot(W) adalah perkalian matriks tanpa menggunakan looping masing-masing element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "w0iICvrvlSpv",
    "outputId": "bef66165-997a-4e3e-caba-4db524e1c111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch ke-1, loss 0.4386541896964116\n",
      "epoch ke-2, loss 0.42822629437941406\n",
      "epoch ke-3, loss 0.41821768452474783\n",
      "epoch ke-4, loss 0.4086053951389583\n",
      "epoch ke-5, loss 0.3993680292858381\n",
      "epoch ke-6, loss 0.39048563090387217\n",
      "epoch ke-7, loss 0.381939569503694\n",
      "epoch ke-8, loss 0.3737124354951006\n",
      "epoch ke-9, loss 0.3657879450390952\n",
      "epoch ke-10, loss 0.3581508534477093\n",
      "epoch ke-11, loss 0.3507868762655683\n",
      "epoch ke-12, loss 0.34368261726453103\n",
      "epoch ke-13, loss 0.33682550266808226\n",
      "epoch ke-14, loss 0.3302037209971499\n",
      "epoch ke-15, loss 0.32380616799496076\n",
      "epoch ke-16, loss 0.3176223961466826\n",
      "epoch ke-17, loss 0.31164256836087656\n",
      "epoch ke-18, loss 0.305857415425122\n",
      "epoch ke-19, loss 0.3002581968883017\n",
      "epoch ke-20, loss 0.2948366650575961\n",
      "epoch ke-21, loss 0.2895850318298155\n",
      "epoch ke-22, loss 0.2844959381047585\n",
      "epoch ke-23, loss 0.27956242555326827\n",
      "epoch ke-24, loss 0.2747779105349251\n",
      "epoch ke-25, loss 0.2701361599801754\n",
      "epoch ke-26, loss 0.2656312690694657\n",
      "epoch ke-27, loss 0.2612576405578217\n",
      "epoch ke-28, loss 0.25700996560755046\n",
      "epoch ke-29, loss 0.25288320600450176\n",
      "epoch ke-30, loss 0.24887257764478005\n",
      "epoch ke-31, loss 0.24497353518910064\n",
      "epoch ke-32, loss 0.24118175779125098\n",
      "epoch ke-33, loss 0.23749313581546302\n",
      "epoch ke-34, loss 0.2339037584650382\n",
      "epoch ke-35, loss 0.23040990225135838\n",
      "epoch ke-36, loss 0.22700802023856415\n",
      "epoch ke-37, loss 0.22369473200473247\n",
      "epoch ke-38, loss 0.22046681426542358\n",
      "epoch ke-39, loss 0.21732119211002474\n",
      "epoch ke-40, loss 0.21425493080545743\n",
      "epoch ke-41, loss 0.21126522812556936\n",
      "epoch ke-42, loss 0.2083494071679542\n",
      "epoch ke-43, loss 0.2055049096230414\n",
      "epoch ke-44, loss 0.20272928946313434\n",
      "epoch ke-45, loss 0.20002020702165052\n",
      "epoch ke-46, loss 0.19737542343516773\n",
      "epoch ke-47, loss 0.19479279542303204\n",
      "epoch ke-48, loss 0.1922702703812398\n",
      "epoch ke-49, loss 0.1898058817691035\n",
      "epoch ke-50, loss 0.18739774476884963\n",
      "epoch ke-51, loss 0.1850440521998021\n",
      "epoch ke-52, loss 0.18274307067018242\n",
      "epoch ke-53, loss 0.18049313695081876\n",
      "epoch ke-54, loss 0.17829265455621982\n",
      "epoch ke-55, loss 0.17614009051953242\n",
      "epoch ke-56, loss 0.17403397234888152\n",
      "epoch ke-57, loss 0.17197288515349193\n",
      "epoch ke-58, loss 0.1699554689288223\n",
      "epoch ke-59, loss 0.16798041599070318\n",
      "epoch ke-60, loss 0.16604646854917593\n",
      "epoch ke-61, loss 0.16415241641338055\n",
      "epoch ke-62, loss 0.16229709481943577\n",
      "epoch ke-63, loss 0.16047938237381493\n",
      "epoch ke-64, loss 0.15869819910522376\n",
      "epoch ke-65, loss 0.15695250461846905\n",
      "epoch ke-66, loss 0.15524129634423733\n",
      "epoch ke-67, loss 0.15356360787911055\n",
      "epoch ke-68, loss 0.1519185074105245\n",
      "epoch ke-69, loss 0.1503050962217165\n",
      "epoch ke-70, loss 0.14872250727203884\n",
      "epoch ke-71, loss 0.14716990384830986\n",
      "epoch ke-72, loss 0.1456464782831533\n",
      "epoch ke-73, loss 0.14415145073653596\n",
      "epoch ke-74, loss 0.14268406803695288\n",
      "epoch ke-75, loss 0.14124360257893262\n",
      "epoch ke-76, loss 0.13982935127374435\n",
      "epoch ke-77, loss 0.13844063455037914\n",
      "epoch ke-78, loss 0.13707679540406212\n",
      "epoch ke-79, loss 0.13573719848971522\n",
      "epoch ke-80, loss 0.13442122925795175\n",
      "epoch ke-81, loss 0.13312829313132554\n",
      "epoch ke-82, loss 0.1318578147186991\n",
      "epoch ke-83, loss 0.1306092370657157\n",
      "epoch ke-84, loss 0.1293820209394852\n",
      "epoch ke-85, loss 0.1281756441457028\n",
      "epoch ke-86, loss 0.12698960087652078\n",
      "epoch ke-87, loss 0.12582340108759704\n",
      "epoch ke-88, loss 0.12467656990282582\n",
      "epoch ke-89, loss 0.12354864704535376\n",
      "epoch ke-90, loss 0.12243918629354972\n",
      "epoch ke-91, loss 0.1213477549606851\n",
      "epoch ke-92, loss 0.1202739333971421\n",
      "epoch ke-93, loss 0.1192173145140368\n",
      "epoch ke-94, loss 0.11817750332720751\n",
      "epoch ke-95, loss 0.11715411652057114\n",
      "epoch ke-96, loss 0.11614678202791186\n",
      "epoch ke-97, loss 0.11515513863221027\n",
      "epoch ke-98, loss 0.11417883558167596\n",
      "epoch ke-99, loss 0.11321753222168571\n",
      "epoch ke-100, loss 0.11227089764187666\n"
     ]
    }
   ],
   "source": [
    "# loop training sesuai epochs\n",
    "\n",
    "for epoch in range (EPOCHS):\n",
    "  preds = sigmoid(X.dot(W))\n",
    "  error = preds - y\n",
    "  loss = np.sum(error ** 2)\n",
    "  # ** pengertiannya adalah pangkat\n",
    "  # loss seperti mencari squared error\n",
    "  lossHistory.append(loss)\n",
    "\n",
    "  print('epoch ke-{}, loss {}'.format(epoch+1, loss))\n",
    "\n",
    "  gradient = X.T.dot(error) / X.shape[0]\n",
    "  #T adalah transpose\n",
    "\n",
    "  #update weight\n",
    "  W = W - ALPHA*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MIjVwCvcmh-X"
   },
   "source": [
    "### APA YANG TERJADI JIKA : NAIKKAN EPOCHS DAN KECILKAN ALPHA ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch ke-1, loss 0.10215795793143749\n",
      "epoch ke-2, loss 0.1020599912800218\n",
      "epoch ke-3, loss 0.10196218148727451\n",
      "epoch ke-4, loss 0.10186452820779121\n",
      "epoch ke-5, loss 0.10176703109713153\n",
      "epoch ke-6, loss 0.10166968981181634\n",
      "epoch ke-7, loss 0.10157250400932427\n",
      "epoch ke-8, loss 0.1014754733480888\n",
      "epoch ke-9, loss 0.10137859748749442\n",
      "epoch ke-10, loss 0.10128187608787395\n",
      "epoch ke-11, loss 0.10118530881050516\n",
      "epoch ke-12, loss 0.10108889531760769\n",
      "epoch ke-13, loss 0.10099263527233976\n",
      "epoch ke-14, loss 0.100896528338795\n",
      "epoch ke-15, loss 0.10080057418199959\n",
      "epoch ke-16, loss 0.10070477246790861\n",
      "epoch ke-17, loss 0.10060912286340357\n",
      "epoch ke-18, loss 0.10051362503628855\n",
      "epoch ke-19, loss 0.10041827865528793\n",
      "epoch ke-20, loss 0.1003230833900426\n",
      "epoch ke-21, loss 0.1002280389111074\n",
      "epoch ke-22, loss 0.1001331448899476\n",
      "epoch ke-23, loss 0.1000384009989363\n",
      "epoch ke-24, loss 0.09994380691135125\n",
      "epoch ke-25, loss 0.09984936230137159\n",
      "epoch ke-26, loss 0.09975506684407506\n",
      "epoch ke-27, loss 0.09966092021543518\n",
      "epoch ke-28, loss 0.09956692209231771\n",
      "epoch ke-29, loss 0.09947307215247844\n",
      "epoch ke-30, loss 0.0993793700745595\n",
      "epoch ke-31, loss 0.09928581553808678\n",
      "epoch ke-32, loss 0.09919240822346707\n",
      "epoch ke-33, loss 0.09909914781198477\n",
      "epoch ke-34, loss 0.09900603398579927\n",
      "epoch ke-35, loss 0.09891306642794208\n",
      "epoch ke-36, loss 0.09882024482231372\n",
      "epoch ke-37, loss 0.09872756885368103\n",
      "epoch ke-38, loss 0.09863503820767416\n",
      "epoch ke-39, loss 0.09854265257078369\n",
      "epoch ke-40, loss 0.09845041163035784\n",
      "epoch ke-41, loss 0.09835831507459991\n",
      "epoch ke-42, loss 0.09826636259256491\n",
      "epoch ke-43, loss 0.09817455387415698\n",
      "epoch ke-44, loss 0.09808288861012693\n",
      "epoch ke-45, loss 0.09799136649206885\n",
      "epoch ke-46, loss 0.09789998721241776\n",
      "epoch ke-47, loss 0.09780875046444665\n",
      "epoch ke-48, loss 0.09771765594226375\n",
      "epoch ke-49, loss 0.0976267033408098\n",
      "epoch ke-50, loss 0.09753589235585536\n",
      "epoch ke-51, loss 0.09744522268399787\n",
      "epoch ke-52, loss 0.09735469402265923\n",
      "epoch ke-53, loss 0.0972643060700829\n",
      "epoch ke-54, loss 0.09717405852533118\n",
      "epoch ke-55, loss 0.09708395108828262\n",
      "epoch ke-56, loss 0.09699398345962934\n",
      "epoch ke-57, loss 0.09690415534087424\n",
      "epoch ke-58, loss 0.09681446643432849\n",
      "epoch ke-59, loss 0.09672491644310882\n",
      "epoch ke-60, loss 0.0966355050711349\n",
      "epoch ke-61, loss 0.0965462320231267\n",
      "epoch ke-62, loss 0.0964570970046018\n",
      "epoch ke-63, loss 0.09636809972187277\n",
      "epoch ke-64, loss 0.09627923988204498\n",
      "epoch ke-65, loss 0.09619051719301339\n",
      "epoch ke-66, loss 0.09610193136346037\n",
      "epoch ke-67, loss 0.09601348210285313\n",
      "epoch ke-68, loss 0.09592516912144083\n",
      "epoch ke-69, loss 0.09583699213025249\n",
      "epoch ke-70, loss 0.09574895084109418\n",
      "epoch ke-71, loss 0.09566104496654654\n",
      "epoch ke-72, loss 0.09557327421996216\n",
      "epoch ke-73, loss 0.09548563831546325\n",
      "epoch ke-74, loss 0.09539813696793911\n",
      "epoch ke-75, loss 0.09531076989304359\n",
      "epoch ke-76, loss 0.09522353680719262\n",
      "epoch ke-77, loss 0.09513643742756171\n",
      "epoch ke-78, loss 0.0950494714720834\n",
      "epoch ke-79, loss 0.09496263865944511\n",
      "epoch ke-80, loss 0.09487593870908637\n",
      "epoch ke-81, loss 0.09478937134119664\n",
      "epoch ke-82, loss 0.09470293627671272\n",
      "epoch ke-83, loss 0.09461663323731641\n",
      "epoch ke-84, loss 0.09453046194543194\n",
      "epoch ke-85, loss 0.09444442212422394\n",
      "epoch ke-86, loss 0.09435851349759462\n",
      "epoch ke-87, loss 0.09427273579018168\n",
      "epoch ke-88, loss 0.09418708872735596\n",
      "epoch ke-89, loss 0.09410157203521875\n",
      "epoch ke-90, loss 0.09401618544059978\n",
      "epoch ke-91, loss 0.09393092867105482\n",
      "epoch ke-92, loss 0.09384580145486332\n",
      "epoch ke-93, loss 0.09376080352102575\n",
      "epoch ke-94, loss 0.09367593459926199\n",
      "epoch ke-95, loss 0.09359119442000835\n",
      "epoch ke-96, loss 0.0935065827144155\n",
      "epoch ke-97, loss 0.09342209921434641\n",
      "epoch ke-98, loss 0.0933377436523737\n",
      "epoch ke-99, loss 0.09325351576177772\n",
      "epoch ke-100, loss 0.093169415276544\n",
      "epoch ke-101, loss 0.09308544193136103\n",
      "epoch ke-102, loss 0.09300159546161821\n",
      "epoch ke-103, loss 0.09291787560340342\n",
      "epoch ke-104, loss 0.09283428209350084\n",
      "epoch ke-105, loss 0.09275081466938866\n",
      "epoch ke-106, loss 0.09266747306923726\n",
      "epoch ke-107, loss 0.09258425703190615\n",
      "epoch ke-108, loss 0.09250116629694279\n",
      "epoch ke-109, loss 0.09241820060457978\n",
      "epoch ke-110, loss 0.09233535969573271\n",
      "epoch ke-111, loss 0.09225264331199814\n",
      "epoch ke-112, loss 0.09217005119565144\n",
      "epoch ke-113, loss 0.09208758308964446\n",
      "epoch ke-114, loss 0.09200523873760375\n",
      "epoch ke-115, loss 0.09192301788382791\n",
      "epoch ke-116, loss 0.0918409202732858\n",
      "epoch ke-117, loss 0.09175894565161447\n",
      "epoch ke-118, loss 0.09167709376511665\n",
      "epoch ke-119, loss 0.09159536436075914\n",
      "epoch ke-120, loss 0.09151375718617023\n",
      "epoch ke-121, loss 0.09143227198963813\n",
      "epoch ke-122, loss 0.09135090852010828\n",
      "epoch ke-123, loss 0.09126966652718174\n",
      "epoch ke-124, loss 0.09118854576111306\n",
      "epoch ke-125, loss 0.09110754597280792\n",
      "epoch ke-126, loss 0.0910266669138214\n",
      "epoch ke-127, loss 0.09094590833635582\n",
      "epoch ke-128, loss 0.09086526999325847\n",
      "epoch ke-129, loss 0.09078475163802022\n",
      "epoch ke-130, loss 0.09070435302477267\n",
      "epoch ke-131, loss 0.09062407390828661\n",
      "epoch ke-132, loss 0.09054391404397027\n",
      "epoch ke-133, loss 0.09046387318786644\n",
      "epoch ke-134, loss 0.09038395109665151\n",
      "epoch ke-135, loss 0.09030414752763266\n",
      "epoch ke-136, loss 0.09022446223874633\n",
      "epoch ke-137, loss 0.09014489498855605\n",
      "epoch ke-138, loss 0.09006544553625076\n",
      "epoch ke-139, loss 0.08998611364164225\n",
      "epoch ke-140, loss 0.089906899065164\n",
      "epoch ke-141, loss 0.08982780156786851\n",
      "epoch ke-142, loss 0.08974882091142597\n",
      "epoch ke-143, loss 0.08966995685812179\n",
      "epoch ke-144, loss 0.08959120917085514\n",
      "epoch ke-145, loss 0.08951257761313658\n",
      "epoch ke-146, loss 0.0894340619490866\n",
      "epoch ke-147, loss 0.08935566194343353\n",
      "epoch ke-148, loss 0.08927737736151138\n",
      "epoch ke-149, loss 0.08919920796925858\n",
      "epoch ke-150, loss 0.08912115353321529\n",
      "epoch ke-151, loss 0.08904321382052226\n",
      "epoch ke-152, loss 0.08896538859891841\n",
      "epoch ke-153, loss 0.08888767763673963\n",
      "epoch ke-154, loss 0.08881008070291607\n",
      "epoch ke-155, loss 0.08873259756697081\n",
      "epoch ke-156, loss 0.08865522799901814\n",
      "epoch ke-157, loss 0.08857797176976143\n",
      "epoch ke-158, loss 0.08850082865049136\n",
      "epoch ke-159, loss 0.0884237984130842\n",
      "epoch ke-160, loss 0.08834688082999984\n",
      "epoch ke-161, loss 0.08827007567428014\n",
      "epoch ke-162, loss 0.08819338271954717\n",
      "epoch ke-163, loss 0.0881168017400011\n",
      "epoch ke-164, loss 0.08804033251041882\n",
      "epoch ke-165, loss 0.08796397480615181\n",
      "epoch ke-166, loss 0.08788772840312456\n",
      "epoch ke-167, loss 0.08781159307783293\n",
      "epoch ke-168, loss 0.08773556860734194\n",
      "epoch ke-169, loss 0.08765965476928456\n",
      "epoch ke-170, loss 0.08758385134185945\n",
      "epoch ke-171, loss 0.08750815810382953\n",
      "epoch ke-172, loss 0.08743257483452035\n",
      "epoch ke-173, loss 0.08735710131381802\n",
      "epoch ke-174, loss 0.08728173732216774\n",
      "epoch ke-175, loss 0.08720648264057179\n",
      "epoch ke-176, loss 0.08713133705058833\n",
      "epoch ke-177, loss 0.0870563003343291\n",
      "epoch ke-178, loss 0.08698137227445818\n",
      "epoch ke-179, loss 0.08690655265419\n",
      "epoch ke-180, loss 0.08683184125728789\n",
      "epoch ke-181, loss 0.08675723786806225\n",
      "epoch ke-182, loss 0.08668274227136887\n",
      "epoch ke-183, loss 0.0866083542526073\n",
      "epoch ke-184, loss 0.08653407359771925\n",
      "epoch ke-185, loss 0.08645990009318684\n",
      "epoch ke-186, loss 0.08638583352603092\n",
      "epoch ke-187, loss 0.08631187368380948\n",
      "epoch ke-188, loss 0.08623802035461609\n",
      "epoch ke-189, loss 0.0861642733270782\n",
      "epoch ke-190, loss 0.08609063239035539\n",
      "epoch ke-191, loss 0.08601709733413798\n",
      "epoch ke-192, loss 0.08594366794864515\n",
      "epoch ke-193, loss 0.08587034402462365\n",
      "epoch ke-194, loss 0.08579712535334585\n",
      "epoch ke-195, loss 0.08572401172660843\n",
      "epoch ke-196, loss 0.08565100293673064\n",
      "epoch ke-197, loss 0.0855780987765527\n",
      "epoch ke-198, loss 0.08550529903943418\n",
      "epoch ke-199, loss 0.0854326035192528\n",
      "epoch ke-200, loss 0.08536001201040219\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "ALPHA = 0.001\n",
    "\n",
    "W=np.random.uniform(size=X.shape[1],)\n",
    "lossHistory = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  pred = sigmoid (X.dot(W))\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range (EPOCHS):\n",
    "  preds = sigmoid(X.dot(W))\n",
    "  error = preds - y\n",
    "  loss = np.sum(error ** 2)\n",
    "  lossHistory.append(loss)\n",
    "\n",
    "  print('epoch ke-{}, loss {}'.format(epoch+1, loss))\n",
    "\n",
    "  gradient = X.T.dot(error) / X.shape[0]\n",
    "\n",
    "  W = W - ALPHA*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terlihat bahwa saat kita naikkan epoch dan turunkan alpha, nilai loss akan semakin kecil. Namun, hal ini mungkin hanya terjadi di kasus ini saja. Bisa jadi, dikasus lain tidak. Oleh karena itu, EPOCHS dan ALPHA adalah hyperparameters yang dapat diubah sesuai dengan kondisi masalah.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction Deep Learning - CNN .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
